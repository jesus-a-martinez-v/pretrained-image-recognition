{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained Networks for Image Recognition\n",
    "\n",
    "---\n",
    "\n",
    "Image recognition is one of the most common tasks in the world of deep neural networks. In fact, one of the fields that has been revolutionized the most by the latest advances in deep learning is computer vision. \n",
    "\n",
    "As we know already, computer vision tasks entail many aspects, from basic image recognition to image segmentation or even autonomous driving. \n",
    "\n",
    "## Why not train our own neural network from scratch?\n",
    "\n",
    "You can totally do it, but it might take a lot of time and computational resources, which translates, most of the time, in considerable amounts of money. \n",
    "\n",
    "For any deep learning solution to pay off, we need lots and lots of training data. This is no different in the realm of computer vision. \n",
    "\n",
    "In many occassions is not easy nor possible to gather enough data. \n",
    "\n",
    "The second aspect to consider is that coming up with novel architectures is hard. It is fat better to leverage existing ones.\n",
    "\n",
    "Finally, training from scratch might take days or even weeks, depending on many factors, such as hardware, money, time, availability, etcetera. \n",
    "\n",
    "## Enter transfer learning\n",
    "\n",
    "Transfer learning is a way to reuse the learned weights of a neural network on a similar tasks. For instance, in this notebook we will create a classifier to tell dogs and cats apart given some photos from the Flickr API. Training a network for this task starting from zero would take ages. Instead, we'll fine-tune the weights of a network pretrained on ImageNet, which would produce faster and better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "Let's first import the libraries we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import flickrapi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing import image as img\n",
    "from keras_applications.vgg16 import preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.python.keras.applications.vgg16 import decode_predictions\n",
    "from config import FLICKR_KEY, FLICKR_SECRET\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from IPython.display import Image as IPImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pretrained network\n",
    "\n",
    "Using Keras we can instantiate many pretrained networks. Keras will download the weights the first time and cache it so it doesn't need to download them again in the future.\n",
    "\n",
    "For instance, let's instantiate a `VGG16` network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the network layers. One thing to notice is the huge amount of parameters it has.\n",
    "\n",
    "Keras comes with many other pretrained models. One of them is Inception, which is a much deeper network but has way less variables, which makes it faster to load an less memory-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = InceptionV3(weights='imagenet', include_top=True)\n",
    "inception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing images\n",
    "\n",
    "Once we have decided on a pretrained model, we need to preprocess the images we're going to feed it.\n",
    "\n",
    "All of pretrained networks in Keras expect their inputs to be of a certain size. They also expect their color channels to be normalized.\n",
    "\n",
    "Let's use PIL to load and center-crop an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and center-crop it\n",
    "image = Image.open('data/pug.jpeg')\n",
    "width, height = image.size\n",
    "square_size = min(width, height)\n",
    "x = (width - square_size) // 2\n",
    "y = (height - square_size) // 2\n",
    "\n",
    "image = image.crop((x, y, square_size, square_size))\n",
    "plt.imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get the network's desired input size it querying the `input_shape` property of its first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizes to the network's required input size.\n",
    "target_square_size = max(dimension for dimension in model.layers[0].input_shape if dimension)\n",
    "image = image.resize((target_square_size, target_square_size), Image.ANTIALIAS)\n",
    "plt.imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to convert the image to a format the network is confortable working with. This typically means we must put our image inside a batch after converting it to an array of numbers and normalizing the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_image = img.img_to_array(image)\n",
    "print(f'Image shape: {numpy_image.shape}')\n",
    "image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "print(f'Image batch shape: {image_batch.shape}')\n",
    "\n",
    "# This is really cool: Each pretrained network comes with a function that \n",
    "# preprocesses images specifically for that architecture.\n",
    "pre_processed = preprocess_input(image_batch, data_format='channels_last')\n",
    "print(f'Pre-processed image shape: {pre_processed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on images\n",
    "\n",
    "Cool, now we can use the model to run inferences on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.predict(pre_processed)\n",
    "print(f'Features shape: {features.shape}')\n",
    "\n",
    "pprint(decode_predictions(features, top=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the model gives us a score for each of the 1000 classes it knows. This makes sense because, remember, we are using a network pretrained on ImageNet; we haven't done any fine-tuning work on it, so it only knows what it learned from ImageNet dataset.\n",
    "\n",
    "We use the function `decode_predictions` to show the features in a more friendly one.\n",
    "\n",
    "The network is pretty sure the photo we gave it was one of a Pug (86.65% confidence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important takeaway of this experiment is that unless we want to make predictions on objects that are part of the ImageNet image corpus, we won't find the pretrained network very useful at first. That's why in the following sections we'll focus on fine-tuning it to make better recognition of cats and dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Flickr API to collect a set of labeled images\n",
    "\n",
    "There are many great resources out there for putting together a set of images for experimentation purposes. One of them is Flickr.\n",
    "\n",
    "To use the Flickr API we need both an application key and secret key. We can get both clicking this link: https://www.flickr.com/services/apps/create\n",
    "\n",
    "From now on, we assume both the key and the secret are in the variables `FLICKR_KEY` and `FLICKR_SECRET` defined in `config.py`.\n",
    "\n",
    "Let's search images using `flickrapi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr = flickrapi.FlickrAPI(FLICKR_KEY, FLICKR_SECRET, format='parsed-json')\n",
    "results = flickr.photos.search(text='cat', per_page='10', sort='relevance')\n",
    "photos = results['photos']['photo']\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the photos returned by Flickr don't contain an URL. Fortunatelly, we can compose it from the record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_flickr_url(photo_record, size=''):\n",
    "    if size:\n",
    "        size = f'_{size}'\n",
    "        \n",
    "    return f'http://farm{photo_record[\"farm\"]}.staticflickr.com/{photo_record[\"server\"]}/{photo_record[\"id\"]}_{photo_record[\"secret\"]}{size}.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easier way to show a bunch of images is using the HTML tag. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [f'<img src=\"{build_flickr_url(photo)}\" width=\"150\" style=\"display:inline\"/>' for photo in photos]\n",
    "\n",
    "HTML('\\n'.join(tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
